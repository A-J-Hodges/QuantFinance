{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Quant Finance\n",
    "\n",
    "## Module 2.3: Testing and Benchmarking\n",
    "\n",
    "### 2.3.1 Statistical Significance\n",
    "\n",
    "When we want to test the results of an experiment, a normal pattern is to setup a Control and treatment group. The terminology comes from medical science, but applies to any field. The Control group is \"do what you do now\". The Treatment group is \"apply my idea to this group\". We then evaluate the different in the two groups to determine if there is a difference or not. Normally referred to as a *significant* difference, but we will come back to that term.\n",
    "\n",
    "Suppose we have a new trading algorithm (we'll call NEW) we want to test. The current one works (named OLD) well enough, but initial testing of NEW indicates some good results. Given the random nature of the stock market, and randomness inherent in both the NEW and OLD algorithms, we can't just run them once and compare the results. In statistics, we always want to ask \"Isn't there some probability that this difference happened by chance?\". Statistics helps us protect against making decisions based on (un)lucky data sampling that happens.\n",
    "\n",
    "We create 30 iterations of both NEW and OLD, and backtest on the last 5 years of trading data. We get the following profits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW = np.array([1.05174941, 1.06930758, 1.11729439, 1.12624468, 1.60443569,\n",
    "       1.16272344, 1.47328065, 1.05485681, 0.74028953, 1.21066169,\n",
    "       1.13720454, 1.70111553, 1.22645839, 1.26188635, 1.30603338,\n",
    "       2.10036382, 1.68648174, 1.27467569, 0.37090243, 1.17720112,\n",
    "       1.25108935, 1.21632526, 1.58731637, 1.08608151, 2.08776142,\n",
    "       0.63474195, 0.70729046, 0.6496959 , 1.61753557, 1.0645431 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD = np.array([0.9847197 , 1.27252081, 0.77785125, 1.53240267, 0.97049964,\n",
    "       1.09014078, 0.92529125, 0.93391001, 1.06337962, 1.04707554,\n",
    "       1.04699074, 1.24765968, 0.97622673, 1.21298906, 1.14389947,\n",
    "       1.08432808, 1.24983952, 0.98100972, 1.34957539, 1.1513302 ,\n",
    "       1.63546461, 0.69778236, 1.46165873, 1.09680951, 1.18708603,\n",
    "       1.04704617, 1.24966216, 0.90329866, 1.41676504, 1.59918173])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2251849250000002, 1.1445464953333333)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW.mean(), OLD.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is great! NEW outperforms OLD. However, isn't there a chance this occured by chance? First, check the histograms. The mean is (usually) the best single-value summary of data, but we can learn so much, so quickly by just doing some quick visualisations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADVFJREFUeJzt3W+MHHUdx/HPR1rEliqtd2AFjoOEEMFowIsBakgFiQhKMfKgRAgQzEUT+WOMBk2EA5/wwCgYNVoRxYjlQQFFAkgDNESR6rUUWih/C0Kl2oMqUDVg8euDncbluD+7O7M3e9++X8lmZ2d+e7/v/Tr5dO43O7OOCAEAZr+31V0AAKAaBDoAJEGgA0ASBDoAJEGgA0ASBDoAJEGgA0ASBDoAJEGgA0ASc2ays76+vhgcHJzJLgFg1lu3bt2LEdE/XbsZDfTBwUGNjo7OZJcAMOvZ/nMr7ZhyAYAkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkZvRKUWA6I3tYv0CVOEIHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIYtpAt32d7e22NzWtW2R7te0ni+eF3S0TADCdVo7QfybplHHrLpV0d0QcLunu4jUAoEbTBnpE3Cdpx7jVyyRdXyxfL+mMiusCALSp0zn0AyJimyQVz/tXVxIAoBNdPylqe9j2qO3RsbGxbncHAHusTgP9b7YXS1LxvH2yhhGxIiKGImKov7+/w+4AANPpNNBvlXRusXyupF9XUw4AoFOtfGxxpaQ/SDrC9lbbF0i6StLJtp+UdHLxGgBQoznTNYiIsybZdFLFtQAASuBKUQBIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIolSg2/6S7Udsb7K90vY+VRUGAGhPx4Fu+0BJF0kaioj3S9pL0vKqCgMAtKfslMscSe+wPUfSPEkvlC8JANCJjgM9Iv4i6VuSnpO0TdLLEXHX+Ha2h22P2h4dGxvrvFIAwJTKTLkslLRM0qGS3itpvu2zx7eLiBURMRQRQ/39/Z1XCgCYUpkpl49JeiYixiLiP5JulnR8NWUBANpVJtCfk3Ss7Xm2LekkSZurKQsA0K4yc+hrJa2StF7SxuJnraioLgBAm+aUeXNEXC7p8opqAQCUwJWiAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASZQKdNv72V5l+zHbm20fV1VhAID2zCn5/msk3RkRZ9reW9K8CmoCAHSg40C3/U5JJ0g6T5Ii4nVJr1dTFgCgXWWmXA6TNCbpp7YftH2t7fkV1QUAaFOZKZc5ko6RdGFErLV9jaRLJX2juZHtYUnDkjQwMFCiO8waIyOdv3fp0jc/A2hZmSP0rZK2RsTa4vUqNQL+TSJiRUQMRcRQf39/ie4AAFPpONAj4q+Snrd9RLHqJEmPVlIVAKBtZT/lcqGkG4pPuGyRdH75kgAAnSgV6BGxQdJQRbUAAErgSlEASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4Akyt4PHRmV+Qq5qqxZU39/vTAOXTSyh/adGUfoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASRDoAJAEgQ4ASZQOdNt72X7Q9m1VFAQA6EwVR+gXS9pcwc8BAJRQKtBtHyTpNEnXVlMOAKBTZb+x6GpJX5W0YLIGtoclDUvSwMBAye6A7hhZurS+vmvrGdl0fIRu+5OStkfEuqnaRcSKiBiKiKH+/v5OuwMATKPMlMsSSafbflbSjZJOtP2LSqoCALSt40CPiK9FxEERMShpuaR7IuLsyioDALSFz6EDQBJlT4pKkiJijaQ1VfwsAEBnOEIHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQIdABIgkAHgCQqudtiZiN7aN8AZh+O0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJIg0AEgCQIdAJLoONBtH2z7XtubbT9i++IqCwMAtKfM/dB3SfpyRKy3vUDSOturI+LRimoDALSh4yP0iNgWEeuL5VclbZZ0YFWFAQDaU8kcuu1BSUdLWlvFzwMAtK/0V9DZ3lfSTZIuiYhXJtg+LGlYkgYGBsp2l9+aNRMvt2pkpHQJI0uXlv4ZKXQy/s2qGMcK/j0n1Wp9ifaHkeT9ljpCtz1XjTC/ISJunqhNRKyIiKGIGOrv7y/THQBgCmU+5WJJP5G0OSK+XV1JAIBOlDlCXyLpHEkn2t5QPE6tqC4AQJs6nkOPiN9JcoW1AABK4EpRAEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJAh0AEiCQAeAJEp/Bd1MGam7gBrwVXCzXItfYTcy1Ub2AbSBI3QASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkCHQASIJAB4AkSgW67VNsP277KduXVlUUAKB9HQe67b0kfV/SJyQdKeks20dWVRgAoD1ljtA/LOmpiNgSEa9LulHSsmrKAgC0q0ygHyjp+abXW4t1AIAalPkKOk+wLt7SyB6WNFy83Gn78RJ9dkufpBfrLqIN1Ntd1NtdfVfMsnpVst4rytdwSCuNygT6VkkHN70+SNIL4xtFxApJK0r003W2RyNiqO46WkW93UW93UW93VNmyuVPkg63fajtvSUtl3RrNWUBANrV8RF6ROyy/UVJv5W0l6TrIuKRyioDALSlzJSLIuJ2SbdXVEudenpKaALU213U213U2yWOeMt5TADALMSl/wCQROpAn+7WBLbPsz1me0Px+FzTtnNtP1k8zu2Rer/TVOsTtv/RtO2Npm0zcnLa9nW2t9veNMl22/5u8fs8bPuYpm11jO909X62qPNh2/fb/mDTtmdtbyzGd7RH6l1q++Wmf/fLmrbN+G05Wqj3K021bir22UXFthkdX9sH277X9mbbj9i+eII2PbX/tiQiUj7UOFH7tKTDJO0t6SFJR45rc56k703w3kWSthTPC4vlhXXXO679hWqciN79emcNY3yCpGMkbZpk+6mS7lDjmoVjJa2ta3xbrPf43XWocUuLtU3bnpXU12Pju1TSbWX3pZmqd1zbT0m6p67xlbRY0jHF8gJJT0yQDz21/7byyHyEXubWBB+XtDoidkTE3yWtlnRKl+rcrd16z5K0sss1TSki7pO0Y4omyyT9PBoekLSf7cWqZ3ynrTci7i/qkaQH1Li2ojYtjO9karktR5v11rr/RsS2iFhfLL8qabPeeqV7T+2/rcgc6K3emuAzxZ9Tq2zvvlCqjtsatNyn7UMkHSrpnqbV+9getf2A7TO6V2ZbJvudZsNtIy5Q4+hst5B0l+11xdXPveI42w/ZvsP2UcW6nh5f2/PUCMCbmlbXNr62ByUdLWntuE2zbv8t9bHFHtfKrQl+I2llRLxm+/OSrpd0YovvrVo7fS6XtCoi3mhaNxARL9g+TNI9tjdGxNOVV9meyX6nOsa3ZbY/qkagf6Rp9ZJifPeXtNr2Y8URaZ3WSzokInbaPlXSryQdrh4fXzWmW34fEc1H87WMr+191fiP5ZKIeGX85gne0tP7b+Yj9GlvTRARL0XEa8XLH0v6UKvv7YJ2+lyucX+uRsQLxfMWSWvUOOKo22S/Ux3j2xLbH5B0raRlEfHS7vVN47td0i1qTGvUKiJeiYidxfLtkuba7lMPj29hqv13xsbX9lw1wvyGiLh5giazbv+tfRK/Ww81/vrYosbUxO4TQ0eNa7O4afnTkh6I/5/0eEaNEx4Li+VFdddbtDtCjRNIblq3UNLbi+U+SU9qBk6CFf0NavKTdqfpzSeV/ljX+LZY74CkpyQdP279fEkLmpbvl3RKD9T7nt37gRoB+Fwx1i3tSzNdb7H9XWrMs8+vc3yLcfq5pKunaNNz++90j7RTLjHJrQlsXylpNCJulXSR7dMl7VJjJzuveO8O299U4341knRlvPnPw7rqlRonk26MYs8qvE/Sj2z/V42/uq6KiEe7Wa8k2V6pxict+mxvlXS5pLnF7/NDNa4iPlWNkPyXpPOLbTM+vi3We5mkd0v6gW1J2hWNmzIdIOmWYt0cSb+MiDt7oN4zJX3B9i5J/5a0vNgvarktRwv1So0Dp7si4p9Nb61jfJdIOkfSRtsbinVfV+M/9Z7cf1vBlaIAkETmOXQA2KMQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQBIEOAEkQ6ACQxP8AOENwPY5JmRcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(OLD, alpha=0.5, color='red')\n",
    "plt.hist(NEW, alpha=0.5, color='aqua');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW has a greater spread, but looks to be generally a bit higher. More profit, but more risk. (Sidenote: normally we would analyse the variance more here, but we are focusing on testing this improvement in this module.)\n",
    "\n",
    "We *could* fit distribution models to the data, determine if the results are normal, fit normal distributions, compare the results using a two-side t-test. This will work for this data, but may not work for other data in the real world.\n",
    "\n",
    "A method that is as rigourous\\*, but more broadly applicable is simulation. You did a Monte Carlo simulation in a previous Extended Exercise - here we will cover a simple pattern that you can use, even if you don't have a simulation environment setup.\n",
    "\n",
    "\\* It is just as rigourous, only if you have sufficient data. The data requirement for simulations is higher than a t-test.\n",
    "\n",
    "## Re-randomised subsets\n",
    "\n",
    "Our data is currently split into two groups - NEW and OLD. Comparing the difference of means, we get this value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0806384296666669"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_difference_in_means = NEW.mean() - OLD.mean()\n",
    "experiment_difference_in_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know - \"is this result impressive?\". More specifically, \"what is the probability that this happened by chance?\".\n",
    "\n",
    "In trading, results can be volatile. This means that we can observe differences this great by just choosing a different subset of stocks, or a different random starting point in our algorithm.\n",
    "\n",
    "To evaluate this, we take *all* the data we have collected so far, and create a new, randomly selected split. We then compute the difference of means. We do this many times, and then see how many resulting in a difference of means at least as great:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_subsets(all_data):\n",
    "    \"\"\"Randomly splits all_data into two groups\"\"\"\n",
    "    # Note this is an in-place operation - all_data is changed by this call!\n",
    "    # If you didn't want this to happen, create a copy of all_data first. We dont' care here though\n",
    "    np.random.shuffle(all_data)\n",
    "    midpoint = int(len(all_data) / 2)\n",
    "    return all_data[:midpoint], all_data[midpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = np.hstack([NEW, OLD])\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1, sample2 = create_random_subsets(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06930758, 0.69778236, 1.05485681, 1.27467569, 0.74028953,\n",
       "       1.16272344, 0.97622673, 0.63474195, 1.70111553, 1.47328065,\n",
       "       1.21632526, 1.59918173, 1.68648174, 0.6496959 , 1.34957539,\n",
       "       1.04699074, 1.08608151, 1.04707554, 1.58731637, 1.12624468,\n",
       "       1.25108935, 1.0645431 , 1.26188635, 1.22645839, 0.37090243,\n",
       "       1.05174941, 2.08776142, 1.46165873, 1.24966216, 0.93391001])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027092055000000004"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1.mean() - sample2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, when you run this, you'll get a value near 0. This is because:\n",
    "\n",
    "* We assume that NEW was generated by one mechanism, just randomly within that mechanism\n",
    "* We assume that OLD was generated by one mechanism (different to NEW)\n",
    "* Our random sample will have about the same number of NEW and OLD values\n",
    "\n",
    "This all averages out, and we expect each sample's mean to be the mean of the whole dataset. Given two samples like this, the difference will be about zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_compare(all_data):\n",
    "    \"\"\"Run a single iteration of (1) split randomly, and (2) compute difference of means\"\"\"\n",
    "    # Same code as above, just in a function\n",
    "    sample1, sample2 = create_random_subsets(all_data)\n",
    "    return sample1.mean() - sample2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do that 1000 times\n",
    "differences = np.array([sample_and_compare(all_data) for _ in range(1000)])\n",
    "\n",
    "# Sidenote: the _ after the word for is a valid variable name.\n",
    "# We use _ as a variable name to indicate to future readers of the program that:\n",
    "# \"This is a variable, but I don't really care about it, its value doesn't matter\".\n",
    "# Often you'll see i here instead - that's perfectly fine too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002141762085333336"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences.mean()  # Expected to be about zero. Your result may vary slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.,  39.,  89., 159., 214., 234., 142.,  76.,  32.,   8.]),\n",
       " array([-0.23655931, -0.18811357, -0.13966783, -0.09122209, -0.04277634,\n",
       "         0.0056694 ,  0.05411514,  0.10256088,  0.15100662,  0.19945236,\n",
       "         0.2478981 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just eyeballing it, you can see *about* 1/3 of the values are above 0.08. We normally test the absolute value - i.e. how likely is it we can an absolute value above the observed one. This gives a clearer picture if we are unsure of which group (control - OLD, or treatment - NEW) is better.\n",
    "\n",
    "Let's find out exactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.336"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(np.abs(differences) > experiment_difference_in_means).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that 33.6% of the values in our simulation are above the observed difference in the data. Therefore we can say that:\n",
    "\n",
    "    There is a 33.6% chance that the observed difference between NEW and OLD occured by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is that difference \"significant\"?\n",
    "\n",
    "A key term you'll hear, and use, when presenting results of statistical tests is whether the result is significant or not. This is a loaded term in statistics, so it is important to distinguish between these two variants:\n",
    "\n",
    "* **Statistical significance** occurs when the Null Hypothesis is unlikely to have occured by chance, given a significance threshold (usually 0.05). That is, if there is a less than 5% chance of the Null hypothesis having occured by chance, we claim this result is **statistically significant**.\n",
    "* A **significant** result is one that causes an improvement that is worth the investment. That is, if a treatment causes an improvement, and we want to use the treatment now, that result is significant.\n",
    "\n",
    "\n",
    "In the above result, we can say that the result of our experiment is **not statistically significant**. That is because there is a 33.6% chance that the difference in the profit between NEW and OLD occured purely by chance.\n",
    "\n",
    "However, when we compare the average profit, NEW has a profit 8 percentage points higher than OLD. If we are happy with the increased risk, then we would say that this increase in profits is **significant** and that we should switch to NEW for our trading strategy.\n",
    "\n",
    "More often than not, a significant result that is not statistically significant is just a case of not enough data. If I generate the data again, using the same mechanism as I did for the data at the start of this module, but with many, many more data points, we'll get a result that is both significant and statistically significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: mu=0.17, std = 0.35\n",
    "# OLD: mu = 0.12, std=0.25\n",
    "N_SAMPLES = 100000\n",
    "NEW_large = 1 + np.random.randn(N_SAMPLES) * 0.35 + 0.17\n",
    "OLD_large = 1 + np.random.randn(N_SAMPLES) * 0.25 + 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_difference_in_means_large = NEW_large.mean() - OLD_large.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04887202765520793"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_difference_in_means_large  # Not as large as before, but 5 percentage points is still good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun our simulations. This takes quite a bit longer than before.\n",
    "all_data_large = np.hstack([NEW_large, OLD_large])\n",
    "differences_large = np.array([sample_and_compare(all_data_large) for _ in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(differences_large) > experiment_difference_in_means_large).mean()  # Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(differences_large) > experiment_difference_in_means_large).sum()  # Absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, *none* of the observed difference in means in our randomised subsets were as high as our experimental difference. (Your results may vary, but you'll get a very low number here, and a percentage nearly 0).\n",
    "\n",
    "This result is both **significant**, in that we expect more money now from the new strategy, and **statistically significant** because it is unlikely this result occured by chance. The data didn't change - we can just had more of it.\n",
    "\n",
    "Note that there are some cases where it isn't just a data issue. In these cases the variance is so large, or the data has a pattern that causes the above to be inconclusive from a statistical perspective. However be mindful of the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A common misconception with p\n",
    "\n",
    "Most people misinterpret what a p value means. Let's say we have our original result of NEW and OLD (the one with 30 samples each). A common misinterpretation of the \"high\" p value is to say there is \"no difference\" between the two samples. If we are measuring a correlation and get a high p value, one might say there is \"no correlation\", even if we do get a linear correlation value, just one that has a high p value. In these cases, one would be wrong to say this.\n",
    "\n",
    "Our observed difference was 0.08 - not \"no difference\". There is just a 33.6% chance that randomly doing the experiment would yeild the same, or higher, result.\n",
    "\n",
    "[This article in Nature](https://www.nature.com/articles/d41586-019-00857-9) gives an example of two experiments on the same treatment - an anti-inflammatory drug. Both experiments tested if there was a correlation betwee using the drug and new-onset atrial fibrillation. One paper concludes the drug is associated with the condition. One paper concludes that it is not associated with the condition. Which is correct? \n",
    "\n",
    "In the \"no association\" experiment, the 95% confidence interval spanned between a decrease of 3% of the risk of the condition to a 48% increase in risk. However, due to the statistical test, the confidence interval included 0, indicating that there is greater than 5% chance that the increased risk observed was due to chance. The Nature article calculated the p value as 0.091.\n",
    "\n",
    "The \"association\" experiment found the 95% confidence interval between a 9% increased risk to a 33% increased risk. As zero is not in the confidence interval, they concluded that there was an assocation.\n",
    "\n",
    "Here is the key finding - both experiments had a mean increased risk of 20%. The second experiment just had more data. An average increased risk of 20% for atrial fibrillation is **significant**. Data sample size indicated that it was not **statistically significant** in the first \"no association\" experiment.\n",
    "\n",
    "There are calls from lots of scientists to stop using the term \"statistically significant\". Further, studies have shown about *half* of papers wrongly interpret their p values. The issue isn't so much with the tests themselves, it is the interpretation of the tests that is the problem, and the common \"intuition\" that \"not statistically significant\" means \"no difference\" or \"not important\".\n",
    "\n",
    "To make matters worse, most people will use 0.05 as their threshold without any consideration for what this means, and whether such a value makes sense. \n",
    "\n",
    "The solution is to be clear with what your test results present, and how they could be used in decision making. Do not use shortcuts like \"statistically significant\", when key decisions (like your p value threshold of 0.05 or your \"95%\" confidence interval) are arbitrarily defined. \n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1. Read the linked Nature article on problems with P values: https://www.nature.com/articles/d41586-019-00857-9\n",
    "2. Write a summary (about two or three sentences) explaining the results of the 30-sample NEW versus OLD result. Be clear on terminology. You can compute confidence intervals as well to present these findings, but again, be clear about what that means.\n",
    "\n",
    "If you are doing this course with a group or partner, get them to check your sentence, and discuss whether you would move to NEW based on the summary you presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
